# -*- coding: utf-8 -*-
"""mBART_translate_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qOxH3U0dLlOPUa6ZlXeLirgbtYQ86DZ3

# Install*
"""

# Tokenizer library 설치
!pip install sentencepiece
!pip install -q transformers

"""# Evn*"""

# imports
import argparse
import os
import random
import collections

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import sentencepiece as spm
import tensorflow as tf

from tqdm.notebook import tqdm, trange

# 환경 설정
args = {
    # random seed value
    "seed": 1234
}
args = argparse.Namespace(**args)
print(args)

# random seed 설정
random.seed(args.seed)
np.random.seed(args.seed)
tf.random.set_seed(args.seed)

# Importing the mBART functions from transformer library
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast

model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-one-to-many-mmt")
tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50-one-to-many-mmt", src_lang="en_XX")

# model.eval()

# google drive mount
from google.colab import drive
drive.mount('/content/drive')

# 뉴스기사 파일 접근
import os
# google drive mount
from google.colab import drive
drive.mount('/content/drive')


nmt_dir = '/content/drive/MyDrive/goorm'
os.listdir(nmt_dir)

import json

# with open("./news_data.json", "r", encoding="utf8") as f:
with open("/content/drive/MyDrive/goorm/news_data.json", "r", encoding="utf8") as f:
  
    json_data = json.load(f)

# print(json_data[0]["category"]) # category 정보를 조회
# print(json_data[0]["headline"]) # category 정보를 조회
# print(json_data[0]["content"]) # category 정보를 조회
# print(json_data[0]["date"]) # category 정보를 조회

text = json_data[0]["content"]
# text = json_data[0]["headline"]

# os.path.isdir(nmt_dir)

"""# Train """

import re

text = re.sub(r"[^\uAC00-\uD7A30-9a-zA-Z\s]", "", text)
print(text)

def translate(text):    
    model_inputs = tokenizer(text, return_tensors = "pt")
    generated_tokens = model.generate(**model_inputs, forced_bos_token_id = tokenizer.lang_code_to_id ["ko_KR"], max_length=512)
    translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
    
    return translation

translate(text)